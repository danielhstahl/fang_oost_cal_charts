\documentclass{article}
\usepackage{amsfonts}
\usepackage{hyperref} 
\begin{document}
\SweaveOpts{concordance=TRUE}
\section{Option Calibration}
\setlength{\parindent}{0cm}

The problem of calibrating option prices to market values (the ``inverse problem'') is non-trivial especially with complex pricing models with many parameters.  A naive approach is to perform optimization by minimizing a distance between the prices provided by the market and the modeled prices by varying the input parameters.  However, this can be computationally intensive.  The problem is not convex and there may be a plethora of local minima. The parameter surface may have many ``flat'' areas leading to unstable parameter solutions.  
\\
\\
In our study, we focus on calibrating models defined in the \href{https://github.com/phillyfan1138/fang_oost_cal_charts/tree/master/docs}{Option Calculation} paper.  We use a jump-diffusion a la \href{http://www.people.hbs.edu/rmerton/optionpricingwhenunderlingstock.pdf}{Merton (1976)} while extending the model to allow for a time-changed diffusion component.  The models thus incorporate popular models like Heston's as a special case.  
\\
\\
The code which runs the results shown below is available at the following Github repo: \href{https://github.com/phillyfan1138/fang_oost_cal_charts}{fang\_oost\_cal\_charts}.  

\section{The model}

Levy processes can be constructed with relatively simple characteristic exponents.  In this paper we will focus on processes with characteristic exponents of the following form:

\[\psi(u)=-\frac{\sigma^2 u^2}{2}+\lambda \left(1-e^{\mu_j+\sigma_j ^2 /2}\right)+\lambda \left(e^{\mu_j u i-\sigma_j ^2 u^2 /2}-1\right)\]
\[=-\frac{\sigma^2 u^2}{2}-\psi_j(-i)+\psi_j(u)\]
Where \(\psi_j(u):=\lambda \left( e^{\mu_j u i-\sigma_j ^2 u^2 /2}-1 \right)\) is the characteristic function of the jump component.  \(\psi(u)\) is the characteristic exponent of a Merton jump-diffusion model.  Following \href{http://faculty.baruch.cuny.edu/lwu/papers/timechangeLevy_JFE2004.pdf}{Carr and Wu (2004)}, we also introduce a random time change so that the characteristic function of the normalized log asset is as follows:

\[\mathbb{E}[e^{ui X_t}]=g(-\psi(u), a, a-\sigma_v \rho u \sigma, \sigma_v, v_0)  \]

Where \(X_t=\mathrm{log}\left(\frac{S_t}{S_0}\right)-rt \), and \(g\) is the moment generating function of an integrated CIR process:

\[g(x, a, \kappa, \sigma_v, v_0)=e^{-b(t)v_0-c(t)}\]
Where 
\[b(t)=2x\left(1-e^{-\delta t}\right)/\left(\delta+\kappa+(\delta-\kappa) e^{-\delta t}\right)\]
\[c(t)=\left(\frac{a}{\sigma^2}\right)\left(2\mathrm{log}\left(1+(\kappa-\delta)\left(1-e^{-\delta t}\right)/2\delta \right) +\left(1-e^{-\delta t}\right)(\kappa-\delta)\right)\]
\[\delta=\sqrt{\kappa^2+2x \sigma_v^2}\]

\section{Calibration}

Calibration has traditionally taken the following form:

\[\min_\theta \sum_k w_k \left(C_k-C(k; \theta)\right)^2 \]
Where \(w_k\) is a weight, \(\theta\) are the parameters describing the (risk-neutral) asset process, \(C_k\) is the observed option prices at log-strike \(k\), and \(C(k, \theta)\) is the modeled price.  
\\
\\
As mentioned in the introduction, this problem is not trivial.  See \href{http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=E58EF2375731921D342B8965E1AA18C9?doi=10.1.1.155.662&rep=rep1&type=pdf}{Cont and Tankov (2006)} for details.  Since we are dealing with Levy processes, we instead consider minimizing the following:
\[\min_\theta ||\psi(u_l; \theta)-\hat{\psi}(u_l; k)||\]

We can borrow from \href{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.348.4044&rep=rep1&type=pdf}{Carr and Madan (1999)} and  \href{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.90.8837&rep=rep1&type=pdf}{Belomestny and Reiss (2006)} to create the estimate \(\hat{\psi}\) from the observed option data:
\[ \mathrm{log}\left(1+iu(iu+1)\int_{-\infty} ^{\infty} e^{uix} O(x) dx\right)=\psi(u-i, t)\]
Where \(O(x)=C_{x+\mathrm{log}(S_0)+rt}/S_0-\left(1-e^{x}\right)^+\) and \(x=k-\mathrm{log}(S_0)-rt\).  Since we do not observe a continuum of option prices in the market, we use a monotonic spline to interpolate the option prices.  To preserve accuracy, we use a two part spline fit.  The first fit uses the realized \(O_k=C_k/S_0-\left(1-e^{k-rT}\right)^+\) from the minimum log strike until the last strike that satisfies \(e^{k}/S_0 < 1\).  The second fit is the log of normalized option prices \(\mathrm{log}\left(C_k/S_0\right)\) and is fit from the last strike that satisfies \(e^{k}/S_0 < 1\) until the maximum log strike.
\\
\\
There are three sources of error in the estimate:
\begin{enumerate}
\item The observation error in the options themselves
\item The error in the spline approximation
\item The error in numerically integrating the observed options prices
\end{enumerate}

\subsection{Spline Error}

\subsubsection{Black Scholes}
We first test the spline error on a Black-Scholes model.  The parameters chosen are \(S=10\), \(r=0\), \(t=1\), \(\sigma=.3\), and the strike array \(4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16\).
This model shows very good alignment when choosing a spline on the log of the option price against the strike as can be seen in the following charts.  Note that the black line is estimated, while the red line is actual.  The red line nearly completely covers the black line.

<<echo=FALSE>>=
require(jsonlite)
#dir=getSrcDirectory()[1]
#print(dir)
@

<<echo=FALSE>>=
plotSpline=function(results){
  k=results$synthetic
  o=results$empirical
  plot(k$strike, k$price, type='l', xlab="Strike Price", ylab="Option Price")
  lines(k$strike, k$actual, col='red')
  points(o$strike, o$actual)
}
plotIntegration=function(integration, type){
  exact=integration[[paste0("exact_", type)]]
  estimate=integration[[paste0("estimate_", type)]]
  plot(integration$u, exact, type='l', xlab="u", ylab="psi(u)", col='blue')
  title(paste0("Actual vs Estimated CF: ", type))
  lines(integration$u, estimate,  col='red')
}
base='./'
#base='~/Documents/Code/rust/fang_oost_cal_charts/docs/'

@

<<fig=T, echo=FALSE>>=
results=fromJSON(readLines(paste0(base, 'spline_black_scholes_test_u.json')))
plotSpline(results)
@


\subsubsection{Heston}

We then fit the spline for a Heston model.  The parameters chosen are the strike array \((95,100,130,150,160,165,170,175,185,190,195,200,210,240,250)\).

<<fig=T, echo=FALSE>>=
results=fromJSON(readLines(paste0(base, 'spline_heston.json')))
plotSpline(results)
@


\subsection{Numerical Integration Error}

\subsubsection{Black Scholes}
The numerical integration error can be seen as follows:

<<fig=T, echo=FALSE>>=
results=fromJSON(readLines(paste0(base, 'integrate_black_scholes_test_u.json')))
plotIntegration(results, "re")
@

<<fig=T, echo=FALSE>>=
plotIntegration(results, "im")
@


While in theory the integration should be valid over all real numbers, it appears that it is only accurate from \((-2\pi,\,2\pi)\).  However, since the function is even, we don't benefit from using all the domain and truncate it from \((0, 2 \pi)\).

\subsubsection{Heston}

The following plot shows the integration comparison for a Heston model.

<<fig=T, echo=FALSE>>=
results=fromJSON(readLines(paste0(base, 'integrate_heston.json')))
plotIntegration(results, "re")

@

<<fig=T, echo=FALSE>>=
plotIntegration(results, "im")
@

The model shows very good alignment.


\subsection{Estimates}

\subsubsection{Black Scholes}
Estimating the characteristic function at points \(u\) between negative and positive \(2\pi\), we use the \href{https://www.cs.tufts.edu/comp/150GA/homeworks/hw3/_reading7\%20Cuckoo\%20search.pdf}{cuckoo search} algorithm to minimize the sum of the norms of the differences between the estimated and modeled characteristic functions.  For a simple model like Black Scholes we can also use gradient descent, however for more complicated models the objective function may have many local minima.  

<<echo=FALSE>>=
results=fromJSON(readLines(paste0(base, 'estimate_black_scholes.json')))
results
@

\subsubsection{Heston}

The search for a Heston model shows the following results:

<<echo=FALSE>>=
results=fromJSON(readLines(paste0(base, 'estimate_heston.json')))
results
@

Note that the objective function's surface can be very ``flat'', leading to high variance in the parameters.

\subsection{Cuckoo Search}
The \href{https://www.cs.tufts.edu/comp/150GA/homeworks/hw3/_reading7\%20Cuckoo\%20search.pdf}{cuckoo search} algorithm is a derivatives-free optimization algorithm which uses Monte Carlo simulations to traverse high-dimensional space.  The algorithm scales well to high dimensions and can achieve an acceptable degree of accuracy quickly.  

\end{document}